import streamlit as st
def app():
    st.title('KLASIFIKASI KOMENTAR YOUTUBE')
    st.header('Metode/Algoritma yang Digunakan')
    st.markdown('Pada pembahasan kali ini akan membahas klasifikasi komentar publik di youtube dengan menggunakan Youtube Data API dengan Algoritma atau Metode klasifikasi yang telah dipelajari pada Matkul Pencarian dan Penambangan Web yang ada pada library Scikit-Learn. Metode-metode yang akan digunakan yaitu :')
    #KNN
    st.subheader('A. KNN (K-Nearest Neighbor)')
    st.markdown('K-Nearest Neighbor (KNN) merupakan salah satu metode yang digunakan dalam menyelesaikan masalah pengklasifikasian. Prinsip KNN yaitu mengelompokkan atau mengklasifikasikan suatu data baru yang belum diketahui kelasnya berdasarkan jarak data baru itu ke beberapa tetangga (neighbor) terdekat. Tetangga terdekat adalah objek latih yang memiliki nilai kemiripan terbesar atau ketidakmiripan terkecil dari data lama. Jumlah tetangga terdekat dinyatakan dengan k. Nilai k yang terbaik tergantung pada data.  Nilai k umumnya ditentukan dalam jumlah ganjil (3, 5, 7) untuk menghindari munculnya jumlah jarak yang sama dalam proses pengklasifikasian. Apabila terjadi dua atau lebih jumlah kelas yang muncul sama maka nilai k menjadi k – 1 (satu tetangga kurang), jika masih ada yang sama lagi maka nilai menjadi k – 2 , begitu seterusnya sampai tidak ditemukan lagi kelas yang sama banyak. Banyaknya kelas yang paling banyak dengan jarak terdekat akan menjadi kelas dimana data yang dievaluasi berada. Dekat atau jauhnya tetangga (neighbor) biasanya dihitung berdasarkan jarak Euclidean (Euclidean Distance). Berikut rumus pencarian jarak menggunakan rumus Euclidian :')
    st.latex(r'''d_i = \sqrt{\sum_{i=1}^{p}(x_2i-x_1i)^{2}}''')
    st.markdown('dengan :  \n $x_1$ \n= sampel data   \n$x_2$ \n= data uji  \ni \n= variabel data  \n$d_i$ \n= jarak  \np \n= dimensi data')
    st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/183px-Map1NN.png","Deskripsi KNN")

    #Naive Bayes
    st.subheader('B. Naive Bayes')
    st.markdown('Naive Bayes adalah teknik sederhana untuk membangun pengklasifikasi, model yang menetapkan label kelas ke instance masalah, direpresentasikan sebagai vektor nilai fitur , di mana label kelas diambil dari beberapa himpunan terbatas. Tidak ada satu algoritme untuk melatih pengklasifikasi semacam itu, tetapi keluarga algoritme berdasarkan prinsip umum: semua pengklasifikasi naif Bayes berasumsi bahwa nilai fitur tertentu tidak bergantung pada nilai fitur lainnya, mengingat variabel kelas. Misalnya, buah dapat dianggap sebagai apel jika berwarna merah, bulat, dan berdiameter sekitar 10 cm. Pengklasifikasi naif Bayes menganggap masing-masing fitur ini berkontribusi secara independen terhadap probabilitas bahwa buah ini adalah apel, terlepas dari kemungkinan apa pun.korelasi antara fitur warna, kebulatan, dan diameter. Dalam banyak aplikasi praktis, estimasi parameter untuk model naive bayes menggunakan metode maximum likelihood ; dengan kata lain, seseorang dapat bekerja dengan model naif Bayes tanpa menerima probabilitas Bayesian atau menggunakan metode Bayesian apa pun.')
    st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ROC_curves.svg/220px-ROC_curves.svg.png","Deskripsi Naive Bayes")
    st.markdown('Terlepas dari desainnya yang naif dan asumsi yang tampaknya terlalu disederhanakan, pengklasifikasi naif Bayes telah bekerja cukup baik dalam banyak situasi dunia nyata yang kompleks. Pada tahun 2004, analisis masalah klasifikasi Bayesian menunjukkan bahwa ada alasan teoretis yang masuk akal untuk kemanjuran pengklasifikasi naif Bayes yang tampaknya tidak masuk akal. Namun, perbandingan komprehensif dengan algoritme klasifikasi lain pada tahun 2006 menunjukkan bahwa klasifikasi Bayes mengungguli pendekatan lain, seperti pohon yang diperkuat atau hutan acak . Keuntungan dari naive bayes adalah hanya membutuhkan sejumlah kecil data pelatihan untuk mengestimasi parameter yang diperlukan untuk klasifikasi.')

    #SVM
    st.subheader('C. SVM (Support Vektor Macchine)')
    st.markdown('SVM adalah salah satu metode prediksi yang paling kuat, yang didasarkan pada kerangka pembelajaran statistik atau teori VC yang diusulkan oleh Vapnik (1982, 1995) dan Chervonenkis (1974). Diberikan satu set contoh pelatihan, masing-masing ditandai sebagai milik salah satu dari dua kategori, algoritma pelatihan SVM membangun sebuah model yang memberikan contoh baru untuk satu kategori atau yang lain, menjadikannya sebagai pengklasifikasi linier biner non- probabilistik (walaupun metode seperti Platt penskalaan ada untuk menggunakan SVM dalam pengaturan klasifikasi probabilistik). SVM memetakan contoh-contoh pelatihan ke titik-titik dalam ruang untuk memaksimalkan lebar celah antara kedua kategori tersebut. Contoh-contoh baru kemudian dipetakan ke dalam ruang yang sama dan diprediksi termasuk dalam kategori berdasarkan di sisi celah mana mereka jatuh. Selain melakukan klasifikasi linier , SVM dapat secara efisien melakukan klasifikasi non-linier menggunakan apa yang disebut trik kernel , yang secara implisit memetakan masukannya ke dalam ruang fitur berdimensi tinggi.')
    st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png","Deskripsi SVM")
    st.markdown('Ketika data tidak diberi label, pembelajaran yang diawasi tidak dimungkinkan, dan diperlukan pendekatan pembelajaran yang tidak diawasi , yang berupaya menemukan pengelompokan alami data ke dalam kelompok, dan kemudian memetakan data baru ke kelompok yang terbentuk ini.')
   
    #Bagging
    st.subheader('D. Bagging Classification')
    st.markdown('Bagging merupakan metode yang dapat memperbaiki hasil dari algoritma klasifikasi machine learning dengan menggabungkan klasifikasi prediksi dari beberapa model. Hal ini digunakan untuk mengatasi ketidakstabilan pada model yang kompleks dengan kumpulan data yang relatif kecil. Bagging adalah salah satu algoritma berbasis ensemble yang paling awal dan paling sederhana, namun efektif. Bagging paling cocok untuk masalah dengan dataset pelatihan yang relatif kecil. Bagging mempunyai variasi yang disebut Pasting Small Votes. cara ini dirancang untuk masalah dengan dataset pelatihan yang besar, mengikuti pendekatan yang serupa, tetapi membagi dataset besar menjadi segmen yang lebih kecil. Penggolong individu dilatih dengan segmen ini, yang disebut bites, sebelum menggabungkannya melalui cara voting mayoritas.')
    st.image("https://upload.wikimedia.org/wikipedia/commons/6/6b/Bagging.png","Deskripsi Bagging")
    st.markdown('Bagging mengadopsi distribusi bootstrap supaya menghasilkan base learner yang berbeda, untuk memperoleh data subset. Sehingga melatih base learners, dan bagging juga mengadopsi strategi aggregasi output base leaner, yaitu metode voting untuk kasus klasifikasi dan averaging untuk kasus regresi.')

    #Stacking
    st.subheader('E. Stacking Classification')
    st.markdown('Stacking merupakan cara untuk mengkombinasi beberapa model, dengan konsep meta learner. dipakai setelah bagging dan boosting. tidak seperti bagging dan boosting, stacking memungkinkan mengkombinasikan model dari tipe yang berbeda. Ide dasarnya adalah untuk train learner tingkat pertama menggunakan kumpulan data training asli, dan kemudian menghasilkan kumpulan data baru untuk melatih learner tingkat kedua, di mana output dari learner tingkat pertama dianggap sebagai fitur masukan sementara yang asli label masih dianggap sebagai label data training baru. Pembelajar tingkat pertama sering dihasilkan dengan menerapkan algoritma learning yang berbeda. Dalam fase training pada stacking, satu set data baru perlu dihasilkan dari classifier tingkat pertama. Jika data yang tepat yang digunakan untuk melatih classifier tingkat pertama juga digunakan untuk menghasilkan kumpulan data baru untuk melatih classifier tingkat kedua. proses tersebut memiliki risiko yang tinggi yang akan mengakibatkan overfitting. sehingga disarankan bahwa contoh yang digunakan untuk menghasilkan kumpulan data baru dikeluarkan dari contoh data training untuk learner tingkat pertama, dan prosedur crossvalidasi.')
    st.image("https://upload.wikimedia.org/wikipedia/commons/d/de/Stacking.png","Deskripsi Stacking")

    #NRandom Forest
    st.subheader('F. Random Forest Classification')
    st.markdown('Random forest (RF) adalah suatu algoritma yang digunakan pada klasifikasi data dalam jumlah yang besar. Klasifikasi random forest dilakukan melalui penggabungan pohon (tree) dengan melakukan training pada sampel data yang dimiliki. Penggunaan pohon (tree) yang semakin banyak akan mempengaruhi akurasi yang akan didapatkan menjadi lebih baik. Penentuan klasifikasi dengan random forest diambil berdasarkan hasil voting dari tree yang terbentuk. Pemenang dari tree yang terbentuk ditentukan dengan vote terbanyak. Pembangunan pohon (tree) pada random forest sampai dengan mencapai ukuran maksimum dari pohon data. Akan tetapi,pembangunan pohon random forest tidak dilakukan pemangkasan (pruning) yang merupakan sebuah metode untuk mengurangi kompleksitas ruang. Pembangunan dilakukan dengan penerapan metode random feature selection untuk meminimalisir kesalahan. Pembentukan pohon (tree) dengan sample data menggunakan variable yang diambil secara acak dan menjalankan klasifikasi pada semua tree yang terbentuk. Random forest menggunakan Decision Tree untuk melakukan proses seleksi. Pohon yang dibangun dibagi secara rekursif dari data pada kelas yang sama. Pemecahan (split) digunakan untuk membagi data berdasarkan jenis atribut yang digunakan. Pembuatan decision tree pada saat penentuan klasifikasi,pohon yang buruk akan membuat prediksi acak yang saling bertentangan. Sehingga,beberapa decision tree akan menghasilkan jawaban yang baik. Random forest merupakan salah satu cara penerapan dari pendekatan diskriminasi stokastik pada klasifikasi. Proses Klasifikasi akan berjalan jika semua tree telah terbentuk.Pada saat proses klasifikasi selesai dilakukan, inisialisasi dilakukan dengan sebanyak data berdasarkan nilai akurasinya. Keuntungan penggunaan random forest yaitu mampu mengklasifiksi data yang memiliki atribut yang tidak lengkap,dapat digunakan untuk klasifikasi dan regresi akan tetapi tidak terlalu bagus untuk regresi, lebih cocok untuk pengklasifikasian data serta dapat digunakan untuk menangani data sampel yang banyak. Proses klasifikasi pada random forest berawal dari memecah data sampel yang ada kedalam decision tree secara acak. Setelah pohon terbentuk,maka akan dilakukan voting pada setiap kelas dari data sampel. Kemudian, mengkombinasikan vote dari setiap kelas kemudian diambil vote yang paling banyak.Dengan menggunakan random forest pada klasifikasi data maka, akan menghasilkan vote yang paling baik.')
    st.image("https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png","Deskripsi Random Forest Classication")

    st.markdown('Untuk dapat mengklasifikasi komentar youtube tersebut dengan menggunakan metode yang dipaparkan diatas dapat dilakukan dengan melakukan tahapan-tahapan berikut.  \n1. Crawling  \n2. Prepocessing  \n3. Pemberian Label  \n4. TF(Term Frequncy)  \n5. Modelling  \nSetelah melalui tahapan-tahapan diatas maka kommentar publik di youtube dapat diketahui akurasi dari klasifikasi tersebut. Tahapan-tahapan tersebut akan dipaparkan lebih jelas pada halaman berikutnya..')